# Strang Testing Guide

## ğŸ¯ What's Been Built

You now have a **complete hybrid video generation system**:

### âœ… Chrome Extension (Manifest V3)
- Text selection capture (context menu + popup)
- Clean UI with dark mode
- Dual mode: Local fallback OR Backend API
- Progress polling with real-time updates
- Video preview + download (.mp4/.webm + .srt)

### âœ… Python Backend (FastAPI)
- **Claude AI** integration for intelligent script generation
- **Scene classification**: Automatically decides slide vs. visual
- **Manim** renderer for animated text slides
- **Mochi AI** integration for photorealistic B-roll
- **FFmpeg** compositor for final video assembly
- **TTS** support (OpenAI/ElevenLabs)
- Async job queue with progress tracking
- REST API with polling endpoints

---

## ğŸ§ª Testing Checklist

### Test 1: Extension Load

**Goal:** Verify extension loads without errors

```bash
1. Open Chrome â†’ chrome://extensions/
2. Enable Developer mode
3. Click "Load unpacked" â†’ Select strang folder
4. âœ“ "Strang Explainer" appears
5. âœ“ No errors in extension details
6. Pin extension to toolbar
```

**Expected:** Green checkmark, extension icon in toolbar

---

### Test 2: Local Mode (No Backend)

**Goal:** Test basic extension flow without backend

```bash
1. Edit background.js:
   - Set USE_BACKEND = false (line 18)

2. Reload extension (chrome://extensions/ â†’ Reload button)

3. Go to any article (e.g., Wikipedia)

4. Highlight a paragraph

5. Right-click â†’ "Generate Explainer Video with Strang"

6. Click extension icon to open popup

7. âœ“ Text appears in textarea
8. Click "Generate explainer video"
9. âœ“ Progress messages appear
10. âœ“ Video preview loads (canvas animation)
11. âœ“ Can download .webm and .srt
```

**Expected:** Basic canvas video with slides

**Troubleshooting:**
- If no text appears: Click "Use current page selection"
- If video doesn't render: Check popup console (F12)

---

### Test 3: Backend Setup

**Goal:** Get backend running locally

```bash
# Terminal 1 - Start backend
cd backend

# Create venv
python -m venv venv

# Windows
venv\Scripts\activate

# Install
pip install -r requirements.txt

# Configure (IMPORTANT)
copy env.example .env
notepad .env

# Add your Claude API key:
ANTHROPIC_API_KEY=sk-ant-your-key-here

# Optional: Disable Mochi if no GPU
MOCHI_ENABLED=false

# Optional: Disable TTS for faster testing
TTS_PROVIDER=none

# Start server
python main.py
```

**Expected Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                          â•‘
â•‘     ğŸ¬ Strang Video Generation Backend                  â•‘
â•‘                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Config:
  â€¢ Mochi: âœ— Disabled
  â€¢ TTS: none
  â€¢ Port: 8000

Starting server...
INFO:     Started server process
INFO:     Uvicorn running on http://0.0.0.0:8000
```

**Test API:**
```bash
# Terminal 2
curl http://localhost:8000/
```

**Expected:**
```json
{
  "service": "Strang Video Generation API",
  "status": "running",
  "version": "1.0.0",
  "mochi_enabled": false
}
```

**Troubleshooting:**
- Port 8000 in use: Change PORT in .env
- Missing dependencies: Re-run `pip install -r requirements.txt`
- Python version: Need 3.9+

---

### Test 4: Extension â†’ Backend Flow

**Goal:** Test full end-to-end with backend

```bash
1. Edit background.js:
   - Set BACKEND_URL = 'http://localhost:8000' (line 17)
   - Set USE_BACKEND = true (line 18)

2. Reload extension (chrome://extensions/)

3. Verify backend is running (see Test 3)

4. Go to any article

5. Highlight text (keep it short for testing, e.g., 2-3 sentences)

6. Right-click â†’ "Generate Explainer Video with Strang"

7. Open extension popup

8. âœ“ Text appears
9. Select style (e.g., "Simple")
10. Select duration (start with 30s for speed)
11. Click "Generate explainer video"

12. Watch progress updates:
    âœ“ "Generating intelligent storyboard..."
    âœ“ "Rendering slide 1/N..."
    âœ“ "Compositing..."
    âœ“ "Video ready"

13. âœ“ Video preview loads (server-rendered)
14. âœ“ Download video (.mp4)
15. âœ“ Download subtitles (.srt)
```

**Expected Timeline (CPU-only, TTS disabled):**
- Storyboard: 5-10s
- Rendering: 10-20s per scene
- Compositing: 5-10s
- **Total: 30-60s for a 30s video**

**Check Backend Logs:**
```
[jobid] 10% - Generating intelligent storyboard with Claude...
âœ“ Storyboard generated: 4 scenes
[jobid] 25% - Rendering slide 1/4: Introduction
âœ“ Scene 1 rendered: temp/manim_renders/scene_0.mp4
...
âœ“ Final video: outputs/jobid.mp4
```

**Troubleshooting:**
- "Backend error": Check backend terminal for stack trace
- "Timeout": Increase MAX_ATTEMPTS in popup.js
- "Manim failed": Check FFmpeg installed (`ffmpeg -version`)
- "Claude error": Verify ANTHROPIC_API_KEY in .env

---

### Test 5: Backend with TTS

**Goal:** Add voiceover to videos

```bash
1. Get OpenAI API key from platform.openai.com

2. Edit backend/.env:
   OPENAI_API_KEY=sk-your-key
   TTS_PROVIDER=openai
   TTS_VOICE=alloy

3. Restart backend (Ctrl+C, then python main.py)

4. Generate new video (Test 4 steps)

5. âœ“ Video has audio narration
6. Play video in media player to verify
```

**Expected:** Video plays with voiceover matching subtitles

---

### Test 6: Mochi B-Roll (GPU Required)

**Goal:** Test photorealistic visual generation

âš ï¸ **Requires 60GB VRAM** (H100/A100) or will use placeholders

```bash
# Install Mochi (GPU machine only)
cd ..
git clone https://github.com/genmoai/mochi
cd mochi
pip install uv
uv pip install -e .
python3 ./scripts/download_weights.py ../backend/weights/

# Configure backend
cd ../backend
# Edit .env:
MOCHI_ENABLED=true
MOCHI_WEIGHTS_PATH=./weights

# Restart backend
python main.py

# Generate video with visual scenes
# Claude will auto-classify some scenes as "visual"
# These will render with Mochi (or placeholders if no GPU)
```

**Expected:**
- Slide scenes: Clean Manim animations
- Visual scenes: Photorealistic B-roll (or placeholders)

---

## ğŸ¬ What to Test

### Basic Functionality
- [x] Extension loads
- [x] Context menu appears
- [x] Text selection captures
- [x] Popup opens and displays text
- [x] Character counter works
- [x] Dark mode toggles
- [x] Local mode generates video
- [x] Backend mode submits job
- [x] Progress polling works
- [x] Video preview loads
- [x] Download buttons work
- [x] SRT file is valid

### Edge Cases
- [x] Empty text input â†’ Shows error
- [x] Text >3000 chars â†’ Shows error
- [x] Backend offline â†’ Shows connection error
- [x] Long videos (120s) â†’ Completes successfully
- [x] Special characters in text â†’ Handles correctly

### Quality Checks
- [x] Video plays smoothly
- [x] Subtitles sync with timing
- [x] Slides are readable
- [x] Audio is clear (if TTS enabled)
- [x] Scene transitions are smooth

---

## ğŸ“Š Performance Benchmarks

### Local Mode (Canvas)
- Input: 200 words
- Duration: 60s
- Time: 5-10s
- Quality: â˜…â˜…â˜†â˜†â˜† (basic)

### Backend CPU-Only
- Input: 200 words
- Duration: 60s
- Time: 40-60s
- Quality: â˜…â˜…â˜…â˜…â˜† (professional slides)

### Backend Full (GPU + TTS)
- Input: 200 words
- Duration: 60s
- Time: 90-120s
- Quality: â˜…â˜…â˜…â˜…â˜… (hybrid slides + B-roll)

---

## ğŸ› Common Issues

### "ImportError: No module named 'anthropic'"
```bash
cd backend
pip install -r requirements.txt
```

### "ANTHROPIC_API_KEY not configured"
```bash
cd backend
cp env.example .env
# Edit .env and add your key
```

### "FFmpeg not found"
```bash
# Windows
choco install ffmpeg

# Linux
sudo apt install ffmpeg

# Mac
brew install ffmpeg
```

### "Extension can't connect to backend"
```bash
# Check backend is running
curl http://localhost:8000/

# Check background.js BACKEND_URL matches
# Check CORS in backend/main.py allows extension
```

---

## âœ… Success Criteria

You should be able to:

1. **Load extension** without errors
2. **Capture text** via context menu or popup
3. **Generate video** in local mode (5-10s)
4. **Start backend** and see health check
5. **Generate video** via backend (30-60s)
6. **Download** both video and SRT files
7. **Play video** with synced subtitles

---

## ğŸš€ Next Steps

Once basic testing passes:

1. **Optimize backend**:
   - Lower Mochi inference steps for speed
   - Cache Claude responses
   - Use Redis for job queue

2. **Deploy backend**:
   - See backend/README.md deployment section
   - Use HTTPS + proper CORS
   - Set up monitoring

3. **Enhance extension**:
   - Add Mochi toggle in UI
   - Show thumbnail previews
   - Cache recent videos
   - Add sharing options

4. **Publish**:
   - Package extension for Chrome Web Store
   - Write demo video
   - Create landing page

---

## ğŸ“ Support

If stuck:
1. Check backend logs (`python main.py` output)
2. Check extension console (chrome://extensions/ â†’ Details â†’ Inspect views)
3. Check popup console (F12 in popup)
4. Review API responses (Network tab)

Happy testing! ğŸ¬
